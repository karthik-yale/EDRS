{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from lvm_prediction import predict\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter bad data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/ks2823/Microbiomap/in_silico_replicates_v7/in_silico_replicates_1.json with 1 niches\n",
      "  No entries were removed from in_silico_replicates_1.json\n",
      "Processing file: /home/ks2823/Microbiomap/in_silico_replicates_v7/in_silico_replicates_2.json with 2 niches\n",
      "  No entries were removed from in_silico_replicates_2.json\n",
      "Processing file: /home/ks2823/Microbiomap/in_silico_replicates_v7/in_silico_replicates_3.json with 3 niches\n",
      "  No entries were removed from in_silico_replicates_3.json\n",
      "Processing file: /home/ks2823/Microbiomap/in_silico_replicates_v7/in_silico_replicates_4.json with 4 niches\n",
      "  No entries were removed from in_silico_replicates_4.json\n",
      "Processing file: /home/ks2823/Microbiomap/in_silico_replicates_v7/in_silico_replicates_5.json with 5 niches\n",
      "  No entries were removed from in_silico_replicates_5.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "files = glob.glob('/home/ks2823/Microbiomap/in_silico_replicates_v7/*.json')\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        num_niches = int(file.split('_')[-1].split('.')[0])\n",
    "    except (ValueError, IndexError):\n",
    "        # Skips files that don't match the expected naming convention\n",
    "        continue\n",
    "\n",
    "    # if num_niches < 5:\n",
    "    #     continue\n",
    "\n",
    "    print(f\"Processing file: {file} with {num_niches} niches\")\n",
    "\n",
    "    data_list = []\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    data_list.append(item)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Skips lines that are not valid JSON\n",
    "                    continue\n",
    "\n",
    "    filtered_data_list = []\n",
    "    for dataset in data_list:\n",
    "        if 'inflow' in dataset:\n",
    "            try:\n",
    "                inflow = np.array(dataset['inflow'])\n",
    "                if inflow.shape[1] == 250:\n",
    "                    filtered_data_list.append(dataset)\n",
    "            except IndexError:\n",
    "                # Skips entries where 'inflow' is not a 2D array\n",
    "                continue\n",
    "\n",
    "    if len(data_list) != len(filtered_data_list):\n",
    "        print(f\"  Removed {len(data_list) - len(filtered_data_list)} entries from {os.path.basename(file)}\")\n",
    "\n",
    "        # with open(file, 'w') as f:\n",
    "        #     for item in filtered_data_list:\n",
    "        #         f.write(json.dumps(item) + '\\n')\n",
    "    else:\n",
    "        print(f\"  No entries were removed from {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 1\n",
      "Max: 0.97, Mean: 0.28, Min: 0.04, Percent under 0.3: 0.66, No. of projects: 50\n",
      "\n",
      "Num niches: 10\n",
      "Max: 0.94, Mean: 0.30, Min: 0.06, Percent under 0.3: 0.72, No. of projects: 50\n",
      "\n",
      "Num niches: 2\n",
      "Max: 1.04, Mean: 0.38, Min: 0.05, Percent under 0.3: 0.42, No. of projects: 50\n",
      "\n",
      "Num niches: 3\n",
      "Max: 0.94, Mean: 0.33, Min: 0.11, Percent under 0.3: 0.62, No. of projects: 50\n",
      "\n",
      "Num niches: 4\n",
      "Max: 1.13, Mean: 0.33, Min: 0.10, Percent under 0.3: 0.60, No. of projects: 50\n",
      "\n",
      "Num niches: 5\n",
      "Max: 0.90, Mean: 0.33, Min: 0.11, Percent under 0.3: 0.52, No. of projects: 50\n",
      "\n",
      "Num niches: 6\n",
      "Max: 1.18, Mean: 0.33, Min: 0.03, Percent under 0.3: 0.64, No. of projects: 50\n",
      "\n",
      "Num niches: 7\n",
      "Max: 1.27, Mean: 0.31, Min: 0.06, Percent under 0.3: 0.66, No. of projects: 50\n",
      "\n",
      "Num niches: 8\n",
      "Max: 1.04, Mean: 0.28, Min: 0.04, Percent under 0.3: 0.68, No. of projects: 50\n",
      "\n",
      "Num niches: 9\n",
      "Max: 1.34, Mean: 0.36, Min: 0.06, Percent under 0.3: 0.54, No. of projects: 50\n",
      "\n",
      "\n",
      "Percentage of projects processed: 100.0\n",
      "\n",
      "Percentage of projects under 0.3: 60.6\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('/home/ks2823/Microbiomap/in_silico_replicates_v7/*.json')\n",
    "num_processed = 0\n",
    "num_under = 0\n",
    "for file in files:\n",
    "    num_niches = int(file.split('_')[-1].split('.')[0])\n",
    "    # if num_niches > 5:\n",
    "    #     continue\n",
    "    print(\"Num niches:\", num_niches)\n",
    "\n",
    "    data_list = []\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_list.append(item)\n",
    "    \n",
    "    best_dist_list = []    \n",
    "    for dataset in data_list:\n",
    "        # print(dataset.keys())\n",
    "        project_id = dataset['label']\n",
    "\n",
    "        best_dist = dataset['best_dist']\n",
    "        best_dist_list.append(best_dist)\n",
    "    best_dist_list = np.array(best_dist_list)\n",
    "    num_processed += best_dist_list.size\n",
    "    num_under += np.sum(best_dist_list<0.3)\n",
    "    print(f'Max: {np.max(best_dist_list):.2f}, Mean: {np.mean(best_dist_list):.2f}, Min: {np.min(best_dist_list):.2f}, Percent under 0.3: {(np.sum(best_dist_list<0.3)/best_dist_list.size):.2f}, No. of projects: {best_dist_list.size}\\n')\n",
    "print(f'\\nPercentage of projects processed: {num_processed/5}')\n",
    "print(f'\\nPercentage of projects under 0.3: {100*num_under/num_processed:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max: 0.97, Mean: 0.61, Min: 0.26, Percent under 0.45: 0.50, No. of projects: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_taylor_slope(data):\n",
    "    # Placeholder for actual slope calculation logic\n",
    "    # Assuming data to be a 2D array-like structure\n",
    "    means = data.mean(axis=0)\n",
    "    variances = data.var(axis=0)\n",
    "    slope = np.polyfit(np.log(means), np.log(variances), 1)[0]\n",
    "    return slope\n",
    "\n",
    "def calc_alpha(data):\n",
    "    return sp.stats.entropy(data, axis=1).mean()\n",
    "\n",
    "def fit_exp(data):\n",
    "    data = data[:5]\n",
    "    data = data/data[0]\n",
    "    model = LinearRegression()\n",
    "    try:\n",
    "        model.fit(np.arange(len(data)).reshape(-1, 1), np.log(data))\n",
    "    except:\n",
    "        return np.nan\n",
    "    exponent = -model.coef_[0]\n",
    "    return exponent\n",
    "\n",
    "def participation_ratio(eigenvalues):\n",
    "    return (np.sum(eigenvalues) ** 2) / np.sum(eigenvalues ** 2)\n",
    "\n",
    "def get_shuffled(data, normalization=True):\n",
    "    permuted_data = np.array([np.random.permutation(x) for x in data.T]).T\n",
    "    if normalization:\n",
    "        permuted_data /= np.sum(permuted_data, axis=1)[:, np.newaxis]\n",
    "    return permuted_data\n",
    "\n",
    "def calc_env_dim(met):\n",
    "    met[met<=0] = 0.1*np.min(met[met>0])\n",
    "    log_met = np.log(met) \n",
    "    \n",
    "    met = met - np.mean(met, axis=0)[np.newaxis, :]\n",
    "    met = met[:, ~(np.std(met, axis=0)==0)]\n",
    "    met_std = np.std(met, axis=0)\n",
    "    assert np.all(met_std!=0), \"Met is not filtered properly\"\n",
    "       \n",
    "    log_met = log_met - np.mean(log_met, axis=0)[np.newaxis, :]\n",
    "    log_met = log_met[:, ~(np.std(log_met, axis=0)==0)]\n",
    "    log_met_std = np.std(log_met, axis=0)\n",
    "    assert np.all(log_met_std!=0), \"Log-Met is not filtered properly\"\n",
    "    \n",
    "    met = met/met_std[np.newaxis, :]\n",
    "    log_met = log_met/log_met_std[np.newaxis, :]\n",
    "    \n",
    "    met_shuffled = get_shuffled(met, normalization=False)\n",
    "    log_met_shuffled = get_shuffled(log_met, normalization=False)\n",
    "    \n",
    "    svd = np.linalg.svd(met, compute_uv=False, full_matrices=False)\n",
    "    svd_log = np.linalg.svd(log_met, compute_uv=False, full_matrices=False)\n",
    "    svd_shuffled = np.linalg.svd(met_shuffled, compute_uv=False, full_matrices=False)\n",
    "    svd_shuffled_log = np.linalg.svd(log_met_shuffled, compute_uv=False, full_matrices=False)\n",
    "    \n",
    "    \n",
    "    env_dim = 1/fit_exp(svd)\n",
    "    env_dim_log = 1/fit_exp(svd_log)\n",
    "    env_pr = participation_ratio(svd)\n",
    "    env_pr_log = participation_ratio(svd_log)\n",
    "    env_shuffled_pr = participation_ratio(svd_shuffled)\n",
    "    env_shuffled_pr_log = participation_ratio(svd_shuffled_log)\n",
    "    env_pr_corrected = env_pr - env_shuffled_pr\n",
    "    env_pr_log_corrected = env_pr_log - env_shuffled_pr_log\n",
    "    \n",
    "    return env_dim, env_dim_log, env_pr, env_pr_log, env_pr_corrected, env_pr_log_corrected\n",
    "\n",
    "def calc_generalists(crm, mean_abu):\n",
    "    # sparsity = np.sum(crm)/crm.size    \n",
    "    # mean_num_resources = crm.shape[1]*sparsity\n",
    "    # weighted_num_resources = np.dot(crm.sum(axis=1), mean_abu)\n",
    "    # weighted_to_mean_ratio = weighted_num_resources/mean_num_resources\n",
    "    \n",
    "    # return weighted_to_mean_ratio\n",
    "    return np.dot(mean_abu, crm.sum(axis=1))/np.mean(crm.sum(axis=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 10\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 2\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 47.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 3\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 4\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 5\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 6\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 7\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 8\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num niches: 9\n",
      "Already processed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 32.02it/s]\n"
     ]
    }
   ],
   "source": [
    "in_silico_checkpoint_file = '/home/ks2823/Microbiomap/checkpoints/in_silico_v7.csv'\n",
    "files = glob.glob('/home/ks2823/Microbiomap/in_silico_replicates_v7/*.json')\n",
    "\n",
    "in_silico_df = pd.DataFrame() \n",
    "\n",
    "# try:\n",
    "#     in_silico_df = pd.read_csv(in_silico_checkpoint_file, index_col=0)\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Checkpoint file not found: {in_silico_checkpoint_file}. Starting with an empty DataFrame.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while reading the checkpoint file: {e}\")\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    num_niches = int(file.split('_')[-1].split('.')[0])\n",
    "    print(\"Num niches:\", num_niches)\n",
    "\n",
    "    processed_projects = []\n",
    "    if not in_silico_df.empty:\n",
    "        processed_projects = list(in_silico_df[in_silico_df['num_niches'] == num_niches]['label'])\n",
    "        print(f'Already processed {len(processed_projects)}')\n",
    "\n",
    "    data_list = []\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_list.append(item)\n",
    "\n",
    "    for dataset in tqdm(data_list):\n",
    "        # print(dataset.keys())\n",
    "        if dataset['best_dist'] > 0.3:\n",
    "            continue\n",
    "        project_id = dataset['label']\n",
    "        \n",
    "        if project_id in processed_projects:\n",
    "            # print(f'Skipping {project_id}')\n",
    "            continue\n",
    "\n",
    "        abu = np.array(dataset['data'])\n",
    "        metadata = np.array(dataset['metadata'])\n",
    "        \n",
    "        inflow = np.array(dataset['inflow'])\n",
    "        total_abu = np.sum(abu, axis=1)\n",
    "        rel_abu = abu/total_abu[:, np.newaxis]\n",
    "        \n",
    "        load_ratio = total_abu.mean()/inflow.mean()\n",
    "\n",
    "        mult_data = np.array([np.random.multinomial(10000, sample) for sample in rel_abu])\n",
    "        rel_mult_data = mult_data/np.sum(mult_data, axis=1)[:, np.newaxis]\n",
    "\n",
    "        mult_survival_idxs = np.mean(rel_mult_data, axis=0) >= 1e-3\n",
    "\n",
    "        preference_list = np.array(dataset['CRM'])[mult_survival_idxs, :]\n",
    "        cosine_similarity = sp.spatial.distance.pdist(preference_list, 'cosine')\n",
    "        preference_list[preference_list > 0] = 1\n",
    "\n",
    "        data = rel_mult_data[:, mult_survival_idxs]\n",
    "        data /= np.sum(data, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        crm = np.array(dataset['CRM'])[mult_survival_idxs, :]\n",
    "        non_zero_idxs = np.sum(crm, axis=0) > 0\n",
    "        \n",
    "        num_resources = non_zero_idxs.size\n",
    "        \n",
    "        crm = crm[:, non_zero_idxs]\n",
    "        crm[crm>0] = 1\n",
    "        \n",
    "        crm_sparsity = np.mean(crm)\n",
    "        mean_num_resources = np.mean(np.sum(crm, axis=1))\n",
    "        \n",
    "        taylor = calc_taylor_slope(data)\n",
    "        alpha = calc_alpha(data)\n",
    "        env_dim, env_dim_log, env_pr, env_pr_log, env_pr_corrected, env_pr_log_correced = calc_env_dim(inflow)\n",
    "        \n",
    "        generalists = calc_generalists(crm, data.mean(axis=0))\n",
    "\n",
    "        output = {\n",
    "            'label': project_id,\n",
    "            'num_niches': num_niches,\n",
    "            'mean_load' : total_abu.mean(),\n",
    "            'var_load' : np.var(total_abu),\n",
    "            'load_ratio': load_ratio,\n",
    "            'mean_resources' : preference_list.sum(axis=1).mean(),\n",
    "            'weighted_pref' : np.dot(data.mean(axis=0), preference_list.sum(axis=1)),\n",
    "            'cosine_similarity': cosine_similarity.mean(),\n",
    "            'taylor': taylor,\n",
    "            'alpha': alpha,\n",
    "            'EnvDim': env_dim,\n",
    "            'EnvDimLog': env_dim_log,\n",
    "            'EnvPr': env_pr,\n",
    "            'EnvPrLog': env_pr_log, \n",
    "            'EnvPrCorr': env_pr_corrected,\n",
    "            'EnvPrLogCorr': env_pr_log_correced,\n",
    "            'best_dist': dataset['best_dist'],\n",
    "            'num_resources': num_resources,\n",
    "            'generalists': generalists,\n",
    "            'crm_sparsity': crm_sparsity,\n",
    "            'mean_num_resources': mean_num_resources,\n",
    "        }\n",
    "        \n",
    "        in_silico_df = pd.concat([in_silico_df, pd.DataFrame([output])], ignore_index=True)\n",
    "        in_silico_df.to_csv(in_silico_checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 46.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "\n",
    "for i in tqdm(range(1, 11)):\n",
    "    # Read the JSON files\n",
    "    df_cg_i = pd.read_json(f'Data/in_silico_v7_results/in_silico_cg/cg_{i}.json', lines=True)\n",
    "    df_cg_null_i = pd.read_json(f'Data/in_silico_v7_results/in_silico_cg_null/cg_null_{i}.json', lines=True)\n",
    "    df_comp_i = pd.read_json(f'Data/in_silico_v7_results/in_silico_comp/comp_{i}.json', lines=True)\n",
    "    # df_bin_pred_i = pd.read_json(f'Data/in_silico_v4_results/in_silico_bin_pred/in_silico_bin_pred_{i}.json', lines=True)\n",
    "    # df_cv = pd.read_json(f'Data/in_silico_v4_results/in_silico_cv_theta/cv_theta_{i}.json', lines=True)\n",
    "    # df_cv = df_cv.drop(columns=['NumNiches'])\n",
    "    df_final_i = pd.merge(df_comp_i, df_cg_i, on='label')\n",
    "    df_final_i = pd.merge(df_final_i, df_cg_null_i, on='label')\n",
    "    # df_final_i = pd.merge(df_final_i, df_bin_pred_i, on='label')\n",
    "    # df_final_i = pd.merge(df_final_i, df_cv, on='label')\n",
    "    \n",
    "    df_final_i['num_niches'] = i\n",
    "    \n",
    "    df_list.append(df_final_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comp</th>\n",
       "      <th>exp_cg</th>\n",
       "      <th>exp_cg_null</th>\n",
       "      <th>num_niches</th>\n",
       "      <th>NicheDim</th>\n",
       "      <th>NicheDim_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRJNA430990</td>\n",
       "      <td>5.226969</td>\n",
       "      <td>0.156737</td>\n",
       "      <td>0.165608</td>\n",
       "      <td>1</td>\n",
       "      <td>6.380111</td>\n",
       "      <td>6.038344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRJEB27564</td>\n",
       "      <td>4.767073</td>\n",
       "      <td>0.069104</td>\n",
       "      <td>0.065894</td>\n",
       "      <td>1</td>\n",
       "      <td>14.471010</td>\n",
       "      <td>15.175921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRJEB36316</td>\n",
       "      <td>5.101199</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.055805</td>\n",
       "      <td>1</td>\n",
       "      <td>19.130015</td>\n",
       "      <td>17.919458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRJNA625750</td>\n",
       "      <td>5.230733</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>0.117332</td>\n",
       "      <td>1</td>\n",
       "      <td>8.497355</td>\n",
       "      <td>8.522854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRJNA530790</td>\n",
       "      <td>4.551835</td>\n",
       "      <td>0.065079</td>\n",
       "      <td>0.068943</td>\n",
       "      <td>1</td>\n",
       "      <td>15.365968</td>\n",
       "      <td>14.504677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label      comp    exp_cg  exp_cg_null  num_niches   NicheDim  \\\n",
       "0  PRJNA430990  5.226969  0.156737     0.165608           1   6.380111   \n",
       "1   PRJEB27564  4.767073  0.069104     0.065894           1  14.471010   \n",
       "2   PRJEB36316  5.101199  0.052274     0.055805           1  19.130015   \n",
       "3  PRJNA625750  5.230733  0.117684     0.117332           1   8.497355   \n",
       "4  PRJNA530790  4.551835  0.065079     0.068943           1  15.365968   \n",
       "\n",
       "   NicheDim_null  \n",
       "0       6.038344  \n",
       "1      15.175921  \n",
       "2      17.919458  \n",
       "3       8.522854  \n",
       "4      14.504677  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(df_list)\n",
    "df['NicheDim'] = 1/df['exp_cg']\n",
    "df['NicheDim_null'] = 1/df['exp_cg_null']\n",
    "df['comp'] = df['comp'].apply(lambda x: np.sum(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3503214332857331 6.973133212696987e-16\n"
     ]
    }
   ],
   "source": [
    "x = 1/df.dropna()['exp_cg']\n",
    "y = df.dropna()['comp']\n",
    "\n",
    "spearman, pval = sp.stats.spearmanr(x, y)\n",
    "print(spearman, pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "in_silico_df = pd.merge(in_silico_df, df, on=['num_niches', 'label'], how='inner')\n",
    "print(len(in_silico_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_silico_df.to_csv(in_silico_checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label  num_niches  mean_load  var_load  load_ratio  mean_resources  \\\n",
      "0   PRJEB27564           1   0.168827  0.000224    0.065755       30.865169   \n",
      "1   PRJEB36316           1   0.258569  0.000312    0.094379       31.134831   \n",
      "2  PRJNA625750           1   0.053563  0.000129    0.020171       30.783784   \n",
      "3  PRJNA530790           1   0.138827  0.000105    0.055727       31.281690   \n",
      "4  PRJNA674420           1   0.063023  0.000042    0.023538       31.222222   \n",
      "\n",
      "   weighted_pref  cosine_similarity    taylor     alpha  ...      EnvPr  \\\n",
      "0      33.034376           0.904395  0.984401  3.056031  ...  23.132705   \n",
      "1      33.732530           0.904124  0.954853  3.315368  ...  23.253927   \n",
      "2      31.782954           0.904449  1.074792  2.363747  ...  24.419539   \n",
      "3      33.056961           0.901581  1.002045  3.034429  ...  22.454519   \n",
      "4      33.301259           0.901601  1.039687  2.882259  ...  23.295154   \n",
      "\n",
      "    EnvPrLog  EnvPrCorr  EnvPrLogCorr  best_dist  num_resources  generalists  \\\n",
      "0  38.071781  -0.065376      0.138836   0.165251            250     1.070280   \n",
      "1  37.097152  -0.059344     -0.073025   0.228112            250     1.083434   \n",
      "2  39.908061  -0.078853     -0.109274   0.241761            250     1.032458   \n",
      "3  39.303212  -0.168211      0.062693   0.198606            250     1.056751   \n",
      "4  38.376511  -0.018393     -0.181631   0.171349            250     1.066588   \n",
      "\n",
      "        comp    exp_cg   NicheDim  \n",
      "0  13.115271  0.069104  14.471010  \n",
      "1  13.806242  0.052274  19.130015  \n",
      "2  16.314817  0.117684   8.497355  \n",
      "3  15.911261  0.065079  15.365968  \n",
      "4  17.374840  0.075669  13.215521  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "in_silico_checkpoint_file = '/home/ks2823/Microbiomap/checkpoints/in_silico_v7.csv'\n",
    "in_silico_df = pd.read_csv(in_silico_checkpoint_file, index_col=0)\n",
    "print(in_silico_df.head())\n",
    "print(len(in_silico_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
